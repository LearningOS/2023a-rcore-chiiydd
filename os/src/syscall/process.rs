//! Process management syscalls

use alloc::sync::Arc;

use crate::{
    config::{MAX_SYSCALL_NUM, PAGE_SIZE},
    loader::get_app_data_by_name,
    mm::{translated_refmut, translated_str, translated_byte_buffer, VirtAddr, MapPermission, VPNRange},
    task::{
        add_task, current_task, current_user_token, exit_current_and_run_next,
        suspend_current_and_run_next, TaskStatus, get_current_task_status, get_current_task_syscall_times, get_current_task_create_time, find_current_task_pagetable, current_insert_framed_area, unmap_the_area, set_task_priority,
    }, timer::get_time_us,
};

#[repr(C)]
#[derive(Debug)]
pub struct TimeVal {
    pub sec: usize,
    pub usec: usize,
}

/// Task information
#[allow(dead_code)]
pub struct TaskInfo {
    /// Task status in it's life cycle
    status: TaskStatus,
    /// The numbers of syscall called by task
    syscall_times: [u32; MAX_SYSCALL_NUM],
    /// Total running time of task
    time: usize,
}

/// task exits and submit an exit code
pub fn sys_exit(exit_code: i32) -> ! {
    trace!("kernel:pid[{}] sys_exit", current_task().unwrap().pid.0);
    exit_current_and_run_next(exit_code);
    panic!("Unreachable in sys_exit!");
}

/// current task gives up resources for other tasks
pub fn sys_yield() -> isize {
    trace!("kernel:pid[{}] sys_yield", current_task().unwrap().pid.0);
    suspend_current_and_run_next();
    0
}

pub fn sys_getpid() -> isize {
    trace!("kernel: sys_getpid pid:{}", current_task().unwrap().pid.0);
    current_task().unwrap().pid.0 as isize
}

pub fn sys_fork() -> isize {
    trace!("kernel:pid[{}] sys_fork", current_task().unwrap().pid.0);
    let current_task = current_task().unwrap();
    let new_task = current_task.fork();
    let new_pid = new_task.pid.0;
    // modify trap context of new_task, because it returns immediately after switching
    let trap_cx = new_task.inner_exclusive_access().get_trap_cx();
    // we do not have to move to next instruction since we have done it before
    // for child process, fork returns 0
    trap_cx.x[10] = 0;
    // add new task to scheduler
    add_task(new_task);
    new_pid as isize
}

pub fn sys_exec(path: *const u8) -> isize {
    trace!("kernel:pid[{}] sys_exec", current_task().unwrap().pid.0);
    let token = current_user_token();
    let path = translated_str(token, path);
    if let Some(data) = get_app_data_by_name(path.as_str()) {
        let task = current_task().unwrap();
        task.exec(data);
        0
    } else {
        -1
    }
}

/// If there is not a child process whose pid is same as given, return -1.
/// Else if there is a child process but it is still running, return -2.
pub fn sys_waitpid(pid: isize, exit_code_ptr: *mut i32) -> isize {
    trace!("kernel::pid[{}] sys_waitpid [{}]", current_task().unwrap().pid.0, pid);
    let task = current_task().unwrap();
    // find a child process

    // ---- access current PCB exclusively
    let mut inner = task.inner_exclusive_access();
    if !inner
        .children
        .iter()
        .any(|p| pid == -1 || pid as usize == p.getpid())
    {
        return -1;
        // ---- release current PCB
    }
    let pair = inner.children.iter().enumerate().find(|(_, p)| {
        // ++++ temporarily access child PCB exclusively
        p.inner_exclusive_access().is_zombie() && (pid == -1 || pid as usize == p.getpid())
        // ++++ release child PCB
    });
    if let Some((idx, _)) = pair {
        let child = inner.children.remove(idx);
        // confirm that child will be deallocated after being removed from children list
        assert_eq!(Arc::strong_count(&child), 1);
        let found_pid = child.getpid();
        // ++++ temporarily access child PCB exclusively
        let exit_code = child.inner_exclusive_access().exit_code;
        // ++++ release child PCB
        *translated_refmut(inner.memory_set.token(), exit_code_ptr) = exit_code;
        found_pid as isize
    } else {
        -2
    }
    // ---- release current PCB automatically
}

/// YOUR JOB: get time with second and microsecond
/// HINT: You might reimplement it with virtual memory management.
/// HINT: What if [`TimeVal`] is splitted by two pages ?
pub fn sys_get_time(_ts: *mut TimeVal, _tz: usize) -> isize {
    trace!(
        "kernel:pid[{}] sys_get_time",
        current_task().unwrap().pid.0
    );
    let _dst_ptrs=translated_byte_buffer(current_user_token(), _ts as * const u8, core::mem::size_of::<TimeVal>());
    
    let current_time=get_time_us();
    let   time_val=&TimeVal{
        usec:current_time % 1_000_000,
        sec:current_time / 1_000_000,
    };
    let src_ptr= time_val as * const TimeVal;
    //  copy the time value to the destination's pointers
    for (index,dst) in _dst_ptrs.into_iter().enumerate(){
        let unit_length=dst.len();
        unsafe{
            dst.copy_from_slice(core::slice::from_raw_parts(src_ptr.wrapping_byte_add(index*unit_length) as * const u8, unit_length));
        }
    }
    0
}

/// YOUR JOB: Finish sys_task_info to pass testcases
/// HINT: You might reimplement it with virtual memory management.
/// HINT: What if [`TaskInfo`] is splitted by two pages ?
pub fn sys_task_info(_ti: *mut TaskInfo) -> isize {
    trace!(
        "kernel:pid[{}] sys_task_info",
        current_task().unwrap().pid.0
    );
    // let time_ms=get_time_ms();

    let _dst_ptrs= translated_byte_buffer(
            current_user_token(),
         _ti as *const u8, core::mem::size_of::<TaskInfo>());
    
    let ref taskinfo=TaskInfo{
        status:get_current_task_status(),
        syscall_times:get_current_task_syscall_times(),
        time: get_time_us()/1000-get_current_task_create_time()/1000,
    };
    debug!("TASKINFO syscall times{:?}",taskinfo.syscall_times);
    debug!("TASK CREATE TIME:{}",get_current_task_create_time());
    debug!("TASKINFO time:{}",taskinfo.time);
    let src_ptr= taskinfo as *const TaskInfo;
    //  copy the time value to the destination's pointers
    for (index,dst) in _dst_ptrs.into_iter().enumerate(){
        let unit_length=dst.len();
        unsafe{
            dst.copy_from_slice(core::slice::from_raw_parts(src_ptr.wrapping_byte_add(index*unit_length) as * const u8, unit_length));
        }
    }

    0
}

/// YOUR JOB: Implement mmap.
pub fn sys_mmap(_start: usize, _len: usize, _port: usize) -> isize {
    trace!(
        "kernel:pid[{}] sys_mmap",
        current_task().unwrap().pid.0
    );
    if (_port& !0x7)!=0{
        return -1;
    }
    // read,write,execute flags are all 0.

    if (_port & 0x7) ==0{
        return -1;
    }
    // start va is not aligned
    if _start&(PAGE_SIZE-1)!=0{
        return -1;
    }

    let start_vpn=VirtAddr::from(_start).floor();
    let end_vpn=VirtAddr::from(_start+_len).ceil();
    for vpn in VPNRange::new(start_vpn, end_vpn) {
        if let Some(pte)=find_current_task_pagetable(vpn){
            if pte.is_valid(){
                return -1;
            }
        }
    }
    println!(" MAP:vpn start:{},vpn ends :{}",start_vpn.0,end_vpn.0);

    current_insert_framed_area(start_vpn.into(), end_vpn.into(),
     MapPermission::from_bits_truncate((_port<<1)as u8)|MapPermission::U);

    0
}

/// YOUR JOB: Implement munmap.
pub fn sys_munmap(_start: usize, _len: usize) -> isize {
    trace!(
        "kernel:pid[{}] sys_munmap",
        current_task().unwrap().pid.0
    );
    // start va is not aligned
    if _start&(PAGE_SIZE-1)!=0{
        return -1;
    }
    println!("UNMAP:vpn start:{},vpn ends :{}",VirtAddr::from(_start).0,VirtAddr::from(_start+_len).ceil().0);

    unmap_the_area(_start, _len)
    

    
}

/// change data segment size
pub fn sys_sbrk(size: i32) -> isize {
    trace!("kernel:pid[{}] sys_sbrk", current_task().unwrap().pid.0);
    if let Some(old_brk) = current_task().unwrap().change_program_brk(size) {
        old_brk as isize
    } else {
        -1
    }
}

/// YOUR JOB: Implement spawn.
/// HINT: fork + exec =/= spawn
pub fn sys_spawn(_path: *const u8) -> isize {
    trace!(
        "kernel:pid[{}] sys_spawn NOT IMPLEMENTED",
        current_task().unwrap().pid.0
    );
    
    let token = current_user_token();
    let path = translated_str(token, _path);
    if let Some(data) = get_app_data_by_name(path.as_str()) {
        let task = current_task().unwrap();
        let newtask=task.spawn(data);
        let pid=newtask.pid.0 as isize;

        add_task(newtask);
        pid 
    } else {
        -1
    }

    // let current = current_task().unwrap();
    // let new_task = current.fork();
    // let new_pid = new_task.pid.0;
    // // modify trap context of new_task, because it returns immediately after switching
    // let trap_cx = new_task.inner_exclusive_access().get_trap_cx();
    // // we do not have to move to next instruction since we have done it before
    // // for child process, fork returns 0
    // trap_cx.x[10] = 0;
    // let token=new_task.get_user_token().clone();


    // let path = translated_str(token, _path);
    // if let Some(data) = get_app_data_by_name(path.as_str()) {
    //     let task =new_task.clone();
    //     task.exec(data);
    // } else {
    //     return -1;
    // }
    // // add new task to scheduler
    // add_task(new_task);
    // new_pid as isize

}

// YOUR JOB: Set task priority.
pub fn sys_set_priority(_prio: isize) -> isize {
    trace!(
        "kernel:pid[{}] sys_set_priority NOT IMPLEMENTED",
        current_task().unwrap().pid.0
    );
    if _prio<=1{
        return -1
    }

    set_task_priority(_prio as usize);
    _prio
}
